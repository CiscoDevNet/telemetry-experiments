{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Driven Telemetry (MDT) diagnosis experiment\n",
    "\n",
    "Prerequiste: Model Driven Telemetry data retrieved from a router, timestamp aligned and merged into a single file (merged.csv). The data is already filtered and only contains numeric counters.\n",
    "\n",
    "This notebook performs the following steps:\n",
    "1) Load the data.\n",
    "2) Pre-process the data.\n",
    "3) Visualize the data as a 2D projection using t-distributed stochastic neighbor embeddings (t-SNE).\n",
    "4) Identify clusters using DBSCAN and associated transitions between the clusters, i.e., the system's change-points.\n",
    "5) Distill the key features/counters that best describe a change-point\n",
    "6) Describe the change-point in natural language - explaining what the issue is what could be done to resolve the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import modules.dataset as ds\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"env\")\n",
    "ds.extract_dataset('./datasets/mdt-demo.tgz', './output')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import modules.mdt.datasets as mdt_ds\n",
    "datasets = mdt_ds.Datasets(datasets_dir='./output')\n",
    "datasets.jupyter_select_dataset_device(select_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDT Merged Data\n",
    "See mdt_data_process notebook for how the merged CSV is curated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import modules.utils as utils\n",
    "from io import StringIO\n",
    "\n",
    "merged_data_fn, _ = datasets.get_input_data_file(\"merged.csv\")\n",
    "\n",
    "df = pd.read_csv(merged_data_fn)  \n",
    "\n",
    "# show number of rows and columns - dimensionality\n",
    "shape = df.shape\n",
    "print(\"dataset dimensions: rows={}, columns={}\".format(shape[0], shape[1]))\n",
    "# display a sample of the dataset, first 10 rows with first 10 columns for each row.\n",
    "utils.displayDataFrame(df.iloc[0:9,0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDT Preprocessed Data\n",
    "See mdt_data_process notebook for how the processed-offline CSV is curated.\n",
    "\n",
    "The nature of the network data collected on routers is multi-variate and very heterogeneous in nature. Some counters are incremental (e.g., packet counts), some are percentages (e.g., CPU usage), with ranges varying (e.g., bytes count in the trillions, or booleans that can only be one or zero). An example of incremental data that ranges in the trillions can be found here.\n",
    "\n",
    "In order to be able to compare information from different sources, preprocessing of the selected dataset include three consecutive steps, operating over the entire timeseries:\n",
    "\n",
    "* Order 1 difference for non-decreasing timeseries\n",
    "* Min-max scaling between 0 and 1\n",
    "* Exponential smoothing (with parameter 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data_fn, _ = datasets.get_input_data_file(\"preprocessed_offline.csv\")\n",
    "\n",
    "df = pd.read_csv(preprocessed_data_fn)\n",
    "\n",
    "# show number of rows and columns - dimensionality\n",
    "shape = df.shape\n",
    "print(\"dataset dimensions: rows={}, columns={}\".format(shape[0], shape[1]))\n",
    "# display a sample of the dataset, first 10 rows with first 10 columns for each row.\n",
    "utils.displayDataFrame(df.iloc[0:9,0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changepoint Detector\n",
    "\n",
    "Detect clusters using DBSCAN and the associated transitions of the system between the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from modules.mdt.data_utils import load_data, ORIGINAL_DATA\n",
    "from modules.mdt.changepoint_detector import ChangepointDetector\n",
    "\n",
    "tstp, dataframe = load_data(preprocessed_data_fn, scale=False, data_selection=ORIGINAL_DATA, ft_regex=\"^(?!.*(time|second)).*\")\n",
    "\n",
    "detector = ChangepointDetector(dataframe, datasets.get_device())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "detector.detect()\n",
    "detector.plot(withEvents=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "detector.plot(withEvents=True)\n",
    "detector.select_changepoints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "The selection problem, i.e., \"which of the many features that change are the most descriptive for the change\", is approached by optimizing an information-theoretic metric, i.e., cross-entropy. The goal here is to find the subset of features that describes best what is changing at the given timestamp. The intuition is that cross-entropy gives both the amount of additional information in the subset, and the divergence of the subset distribution from the original one. The added regularization term also allows for the tuning of the verbosity of the output.\n",
    "\n",
    "More details can be found in T. Feltin, J. A. C. Fuertes, F. Brockners and T. H. Clausen, [\"Understanding Semantics in Feature Selection for Fault Diagnosis in Network Telemetry Data‚Äù](https://www.researchgate.net/publication/371814291_Understanding_Semantics_in_Feature_Selection_for_Fault_Diagnosis_in_Network_Telemetry_Data), NOMS 2023 - 2023 IEEE/IFIP Network Operations and Management Symposium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from modules.mdt.retriever import Retriever\n",
    "import modules.utils as utils\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "tstp, dataframe = load_data(merged_data_fn, scale=False, data_selection=ORIGINAL_DATA, ft_regex=\"^(?!.*(time|second|minute|hour|pid|port)).*\",\n",
    "                            remove_nan=True, remove_inf=True)\n",
    "\n",
    "selected_changepoints = detector.get_changepoints()\n",
    "retriever = Retriever(dataframe)\n",
    "features = retriever.retrieve(selected_changepoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mdt_changepoints = []\n",
    "\n",
    "for feature, data in features.items():\n",
    "    mdt_changepoints.append({\n",
    "        \"Event\": f\"{feature - tstp[0]}\",\n",
    "        \"Features\": '\\n'.join(data),\n",
    "        \"Source\": \"MDT\",\n",
    "        'Type': \"NETWORK_DEVICE\"\n",
    "    })\n",
    "\n",
    "clear_output()\n",
    "utils.displayDictionary(mdt_changepoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changepoint / Feature Diagnoser\n",
    "\n",
    "Leverage an LLM to turn the selected set of features along with the amplitude of change into a diagnosis and resolution in natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from modules.diagnose import *\n",
    "from modules.logger import Logger\n",
    "from modules.llm.azure_ai import AzureLlm\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logger = Logger(logging.INFO)\n",
    "llm = AzureLlm(logger,os.getenv('AZURE_OPENAI_API_KEY'))\n",
    "        \n",
    "diagnoser = Diagnose(logger, llm)\n",
    "diagnoser.setOutputInitialDiagnosis(\"Diagnosis\")\n",
    "\n",
    "diagnoser.run(mdt_changepoints, inject=True)\n",
    "utils.displayDictionary(mdt_changepoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-diagnosis",
   "language": "python",
   "name": "ai-diagnosis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
