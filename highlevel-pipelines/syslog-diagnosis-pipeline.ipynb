{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b880b25-8dea-4aa4-a571-6b604c3175b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Log diagnosis experiment\n",
    "\n",
    "This notebook performs the following steps:\n",
    "1) Load the data\n",
    "2) Use an LLM to detect source file types.\n",
    "3) Filter the data in the file to retrieve the \"most relevant\" log lines (this notebook uses a statistical, TF-IDF-based approach).\n",
    "4) Use a LLM to diagnose selected log lines (from step 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294b0c23-43a4-4908-8419-dcad6dac0ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0aa046-7504-46b3-848e-e23d419cccbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define globals and utils\n",
    "\n",
    "Below we define some utility functions to read a .tgz dataset from S3 (or local machine), a custom tokenizer (using Porter Stemmer), stopwords, etc...\n",
    "\n",
    "To run this notebook with another dataset just change the variable `file` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e36bdbb-60a2-4def-8b1e-50c8c2f122c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from modules.logger import Logger\n",
    "from modules.llm.azure_ai import AzureLlm\n",
    "from modules.dataset import load_dataset_local\n",
    "from modules.syslog.tfidf import get_stemmed_tokens, get_stop_words, get_tokens\n",
    "import logging\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"env\")\n",
    "\n",
    "file = './datasets/syslog-demo.tar'\n",
    "\n",
    "\n",
    "STOPWORDS_TO_IGNORE = [\n",
    "    'down'\n",
    "]\n",
    "\n",
    "logger = Logger(logging.INFO)\n",
    "llm = AzureLlm(logger, os.getenv('AZURE_OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95360963-0dac-4509-83a0-86ca49348b81",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1 - Use LLM to infer file type\n",
    "\n",
    "In this section, a LLM is used to detect the file types of our data sources. The accepted types are `syslog` and `configuration`. If the LLM can't find out the category (e.g., the file is empty, or does not contain enough information for the classification), the type `unknown` is used.\n",
    "\n",
    "For longer files (more than x characters), the **first n** characters + **middle n** charactehers and **last n** characters are used as input for the LLM prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8d7212-8b5c-4324-918d-36d55aa6bf4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from modules.llm.prompt import FileTypeDetectionPrompt\n",
    "\n",
    "def augment_file_type(dataset, x=500, n=100):\n",
    "\n",
    "    for data in dataset:\n",
    "        content = data['content']\n",
    "        file_name = data['path']\n",
    "        \n",
    "        # Check if the content length exceeds the threshold 'x'\n",
    "        if len(content) > x:\n",
    "            # Extract first 'n' characters, 'n' middle characters, and last 'n' characters\n",
    "            first_part = content[:n]\n",
    "            middle_part = content[len(content)//2 - n//2:len(content)//2 + n//2]\n",
    "            last_part = content[-n:]\n",
    "            content = first_part + \"\\n...\\n\" + middle_part + \"\\n...\\n\" + last_part\n",
    "\n",
    "        prompt = FileTypeDetectionPrompt(logger, llm)\n",
    "        prompt.run(file_name=file_name, file_content=content)\n",
    "        file_type = prompt.getType()\n",
    "        data['type'] = file_type\n",
    "        print(f\"{file_name} -> {file_type}\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d213c1-7981-4dc9-89cf-dc1c528c0f36",
   "metadata": {},
   "source": [
    "### Load dataset and augment with file types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394f1b8e-c5c0-450d-bebb-46317fe891c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "dataset = load_dataset_local(file)\n",
    "dataset = augment_file_type(dataset=dataset, x=10000, n=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbbc894-d5ef-40f3-8777-a91a9cd4bffe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2 - Compute TF-IDF on all documents\n",
    "\n",
    "The approach is straightforward. For each file (for sake of comparison, also configuration files are considered here) tf-idf scores are computed such that each row of the document-term matrix contains the weights of the tokens of each log line.\n",
    "\n",
    "For each line (row) we aggretate such score and compute an avarage score. Then, we sort all lines by such avarage score (highest to lowest) to so have the most important lines (n) at the top of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab7a22b-1aec-4af7-9a00-f960d8b7fbc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TF-IDF log sorting approach inspired by https://github.com/ExceptionalHandler/NLP/blob/master/README.md\n",
    "\n",
    "import re\n",
    "stopwords = get_stop_words(\"stopwords.txt\")\n",
    "stopwords = list(set(ENGLISH_STOP_WORDS).union(stopwords).difference(STOPWORDS_TO_IGNORE))\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=get_tokens, stop_words=stopwords)\n",
    "tok = '_tok'\n",
    "\n",
    "tf_idf_transformer = TfidfTransformer()\n",
    "\n",
    "def getFeatures(lines, scores, top_n):\n",
    "    line_scores = list(zip(lines, scores))\n",
    "    sorted_lines = sorted(line_scores, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    return [line for line, score in sorted_lines]\n",
    "\n",
    "def process(path, content, top_n):\n",
    "    lines = content.splitlines()\n",
    "    numbered_lines = [f'{i+1:04}: {lines[i]}' for i in range(len(lines))]\n",
    "        \n",
    "    try:\n",
    "        doc_matrix = vectorizer.fit_transform(lines)\n",
    "        tfidf_matrix = tf_idf_transformer.fit_transform(doc_matrix).toarray()\n",
    "        per_line_scores = [row.sum()/len(row.nonzero()[0]) if row.nonzero()[0].size > 0 else 0 for row in tfidf_matrix]\n",
    "            \n",
    "        return numbered_lines, getFeatures(numbered_lines, per_line_scores, top_n), getFeatures(lines, per_line_scores, top_n)\n",
    "    except ValueError as e:\n",
    "        # Return an empty list\n",
    "        print(f\"Skipping file {path}: {e}\")\n",
    "        return numbered_lines,[]\n",
    "\n",
    "results = []\n",
    "results_plt = {}\n",
    "for data in dataset:\n",
    "    numbered_lines, numbered_featured, features = process(data['path'], data['content'], 10)\n",
    "    \n",
    "    results.append({\n",
    "        \"file_path\": data['path'],\n",
    "        \"file_type\": data['type'],\n",
    "        \"snippet\": '\\n'.join(features)\n",
    "    })\n",
    "    \n",
    "    results_plt[data['path']]={\n",
    "        \"file content\": '\\n'.join(numbered_lines[0:30]),\n",
    "        \"retrieved features\": '\\n'.join(numbered_featured)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6439b1",
   "metadata": {},
   "source": [
    "### Show log features retrieved using TF/IDF analysis\n",
    "\n",
    "First select a file that has been processed with available results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d830f3-e29c-4163-a9bb-b3c193d550f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import modules.utils as utils\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "options = list(results_plt.keys())\n",
    "file_select = widgets.Dropdown(options=options,\n",
    "                               value=options[0],\n",
    "                               description = \"select file\",\n",
    "                               disabled = False,\n",
    "                               layout={'width': '600px'})\n",
    "display(file_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360bbfbc",
   "metadata": {},
   "source": [
    "### Show the raw file content with the retrieved features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87edf2a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.displayDictionary(results_plt[file_select.value])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399545f9-bba5-41bf-a937-f155e54b77c6",
   "metadata": {},
   "source": [
    "## 3 - Use a LLM to perform an initial diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda08bce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from modules.diagnose import *\n",
    "\n",
    "diagnose = Diagnose(logger, llm)\n",
    "diagnose.setInputFeatures('snippet')\n",
    "diagnose.setInputSource('file_type')\n",
    "diagnose.setInputFile('file_path')\n",
    "diagnose.setInputType('type')\n",
    "diagnose.setOutputIssueState('issue')\n",
    "diagnose.setOutputIssueDesc('description')\n",
    "diagnose.setOutputResolution('resolution')\n",
    "\n",
    "for row in results: \n",
    "   snippet = row['snippet']\n",
    "   file_path = row['file_path']\n",
    "   file_type = row['file_type']\n",
    "   if file_type != 'syslog':\n",
    "      print(f'\\n\\nIgnoring non-syslog file: {file_path} (type: {file_type})')\n",
    "      row['issue'] = \"IGNORED\"\n",
    "      continue\n",
    "   if len(snippet) == 0:\n",
    "      print(f'\\n\\nIgnoring empty file: {file_path} (type: {file_type})')\n",
    "      row['issue'] = \"IGNORED\"\n",
    "      continue\n",
    "       \n",
    "   if 'router' in file_path:\n",
    "      row['type'] = diagnose.getNetworkDeviceType()\n",
    "   elif 'pod' in file_path:\n",
    "      row['type'] = diagnose.getPodType()\n",
    "   else:\n",
    "      row['type'] = diagnose.getGenericType()\n",
    "\n",
    "diagnose.run(results, inject=True)  \n",
    "print('Diagosis Complete!')\n",
    "\n",
    "utils.displayDictionary(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3db1593-28c4-42f8-9645-7c88a252190d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-diagnosis",
   "language": "python",
   "name": "ai-diagnosis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
