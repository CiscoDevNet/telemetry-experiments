{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnosing Model Driven Telemetry timeseries\n",
    "\n",
    "This notebook loads an MDT dataset and uses \"semantic feature selection\" to choose the most \"relevant\" features for a change point.\n",
    "For details, see T. Feltin, J. A. C. Fuertes, F. Brockners and T. H. Clausen, [\"Understanding Semantics in Feature Selection for Fault Diagnosis in Network Telemetry Data‚Äù](https://www.researchgate.net/publication/371814291_Understanding_Semantics_in_Feature_Selection_for_Fault_Diagnosis_in_Network_Telemetry_Data), NOMS 2023 - 2023 IEEE/IFIP Network Operations and Management Symposium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.dataset as ds\n",
    "ds.extract_dataset('./datasets/mdt-demo.tgz', './output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.mdt.datasets as mdt_ds\n",
    "\n",
    "datasets = mdt_ds.Datasets(datasets_dir='./output')\n",
    "datasets.jupyter_select_dataset_device(select_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Dataset Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import modules.utils as utils\n",
    "\n",
    "data_fn, _ = datasets.get_input_data_file(\"merged.csv\")\n",
    "\n",
    "df = pd.read_csv(open(data_fn, 'rb'))  \n",
    "\n",
    "utils.displayDataFrame(df.iloc[0:9,0:9])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions (load data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "MIN_TIMESTAMP = -62135596800\n",
    "MAX_TIMESTAMP = 253402214400\n",
    "\n",
    "ORIGINAL_DATA     = \"original data\"\n",
    "REDUCED_DATA      = \"reduced data\"\n",
    "FIRST_DERIVATIVE  = \"first derivative\"\n",
    "SECOND_DERIVATIVE = \"second derivative\"\n",
    "\n",
    "def get_feature_names_bis(path, delimiter=','):\n",
    "    \"a more direct and simpler implementation than get_feature_names()\"\n",
    "    with open(path, \"r\") as f:\n",
    "        header = f.readline().strip('\\n')\n",
    "    return header.split(delimiter)\n",
    "\n",
    "def scale_data(d):\n",
    "    d = d - np.mean(d, axis=0)\n",
    "    ft_scale = np.std(d, axis=0)\n",
    "    z_index = np.where(ft_scale < 1e-6)\n",
    "    ft_scale[z_index] = 1\n",
    "    d = d / ft_scale\n",
    "    return d\n",
    "\n",
    "def load_data(in_fn, reduced=None, startTime=MIN_TIMESTAMP, endTime=MAX_TIMESTAMP, \n",
    "              scale=False, data_selection={}, ft_regex=None, remove_nan=False, remove_inf=False) -> (np.array, pd.DataFrame):\n",
    "    data = np.genfromtxt(in_fn, dtype=float, delimiter=',', skip_header=1)\n",
    "\n",
    "    if isinstance(data_selection, str):\n",
    "        selection = {\n",
    "            ORIGINAL_DATA    : False,\n",
    "            REDUCED_DATA     : False,\n",
    "            FIRST_DERIVATIVE : False,\n",
    "            SECOND_DERIVATIVE: False\n",
    "        }\n",
    "        selection[data_selection] = True\n",
    "        data_selection = selection\n",
    "\n",
    "    tstp = data[:,0]\n",
    "    data = data[:,1:]\n",
    "    ft_names = np.asarray(get_feature_names_bis(in_fn)[1:])\n",
    "    if ft_regex:\n",
    "        ft_filter = re.compile(ft_regex, re.IGNORECASE)\n",
    "        ft_idx = np.array([i for i, v in enumerate(map(ft_filter.match, ft_names)) if v is not None])\n",
    "        if len(ft_idx) > 0:\n",
    "            data = data[:, ft_idx]\n",
    "            ft_names = ft_names[ft_idx]\n",
    "        else:\n",
    "            data = np.array([])\n",
    "            ft_names = np.array([])\n",
    "\n",
    "    if remove_nan:\n",
    "        inval_col = np.where(np.any(np.isnan(data), axis=0))\n",
    "        data = np.delete(data, inval_col, axis=1)\n",
    "        ft_names = np.delete(ft_names, inval_col)\n",
    "\n",
    "    if remove_inf:\n",
    "        inval_col = np.where(np.any(np.isinf(data), axis=0))\n",
    "        data = np.delete(data, inval_col, axis=1)\n",
    "        ft_names = np.delete(ft_names, inval_col)\n",
    "    \n",
    "    if scale:\n",
    "        data = scale_data(data)\n",
    "    \n",
    "    final_names = np.asarray([])\n",
    "    final_data = np.array([[] for _ in range(len(data))])\n",
    "    derivative = None\n",
    "    if data_selection[FIRST_DERIVATIVE] or data_selection[SECOND_DERIVATIVE]:\n",
    "        derivative = np.diff(data, axis=0)\n",
    "\n",
    "    if data_selection[ORIGINAL_DATA]:\n",
    "        final_data = np.append(final_data, data, axis=1)\n",
    "        final_names = np.append(final_names, ft_names)\n",
    "    \n",
    "    if data_selection[REDUCED_DATA]:\n",
    "        final_data = np.append(final_data, reduced, axis=1)\n",
    "        final_names = np.append(final_names, [f\"{x}_bytes-sent_reduced\" for x in range(len(reduced[0]))])\n",
    "\n",
    "    if data_selection[FIRST_DERIVATIVE]:\n",
    "        final_data = np.append(final_data, np.vstack([derivative[0,:], derivative]), axis=1)\n",
    "        final_names = np.append(final_names, [f\"{x}_bytes-send_deriv\" for x in ft_names])\n",
    "\n",
    "    if data_selection[SECOND_DERIVATIVE]:\n",
    "        second_derivative = np.diff(derivative, axis=0)\n",
    "        second_derivative = np.vstack([second_derivative[0,:], second_derivative[0,:], second_derivative])\n",
    "        final_data = np.append(final_data, second_derivative, axis=1)\n",
    "        final_names = np.append(final_names, [f\"{x}_bytes-sent_deriv2\" for x in ft_names])\n",
    "\n",
    "    # add timestamp            \n",
    "    final_data = np.append(tstp.reshape(-1,1), final_data, axis=1)\n",
    "    final_names = np.append(np.asarray('ts'), final_names)\n",
    "\n",
    "    # filter by time\n",
    "    if isinstance(startTime, datetime):\n",
    "        startTime = startTime.replace(tzinfo=timezone.utc).timestamp()\n",
    "    if isinstance(endTime, datetime):\n",
    "        endTime = endTime.replace(tzinfo=timezone.utc).timestamp()\n",
    "    final_data = final_data[\n",
    "        (final_data[:,0] >= startTime) &\n",
    "        (final_data[:,0] <= endTime)\n",
    "    ]\n",
    "    final_tstp = final_data[:,0]\n",
    "\n",
    "    return final_tstp, pd.DataFrame(final_data, columns=final_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever Relevant Features\n",
    "\n",
    "Note: For a selection for other change-points adjust ``timestamp = 4820`` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from modules.mdt import explain_lib as explib\n",
    "from modules.mdt.traffic_leaf_classifier import traffic_leaf_test\n",
    "from modules.mdt.selection_lib import run_opti\n",
    "from modules.mdt.feature_store import FeatureStore\n",
    "from modules.mdt.utils import minmax, adaptive_diff, ft_dissect\n",
    "import modules.utils as utils\n",
    "\n",
    "tstp, dataframe = load_data(data_fn, scale=False, data_selection=ORIGINAL_DATA, ft_regex=\"^(?!.*(time|second|minute|hour|pid|port)).*\",\n",
    "                            remove_nan=True, remove_inf=True)\n",
    "\n",
    "one_sided_window = 150\n",
    "max_selection_size = 5\n",
    "regularization_term = 2\n",
    "max_number_of_epochs = 20\n",
    "traffic_metric_name = 'sort_metric_step_eyeQ'\n",
    "timestamp = 4820\n",
    "\n",
    "# get metric func\n",
    "metric_func = getattr(explib, traffic_metric_name)\n",
    "\n",
    "fulldata = dataframe.to_numpy(dtype=float)\n",
    "tstp = fulldata[:,0]\n",
    "data = fulldata[:,1:]        \n",
    "ft_names = dataframe.columns.values[1:]\n",
    "\n",
    "ft_store = FeatureStore(ft_names)\n",
    "ft_names_idx = list(range(len(ft_names)))\n",
    "\n",
    "ft_class = []  # type of each feature\n",
    "# leaf count associate to each kv\n",
    "mainft_leaf_cnt = defaultdict(int)\n",
    "traffic_leaf_cnt = defaultdict(int)\n",
    "\n",
    "onbox_name_format = '[' in ft_store.get_flat_name(0)\n",
    "\n",
    "for fti in ft_names_idx:\n",
    "    if onbox_name_format:\n",
    "        leaf = ft_store.get_joined_path(fti)\n",
    "        kv_str = ft_store.get_joined_kv(fti)\n",
    "    else:\n",
    "        _, _, kv, leaf = ft_dissect(ft_store.get_flat_name(fti))\n",
    "        kv_str = ':'.join(kv)\n",
    "\n",
    "    if traffic_leaf_test(leaf):\n",
    "        ft_class.append(1)\n",
    "        traffic_leaf_cnt[kv_str] += 1\n",
    "    else:\n",
    "        ft_class.append(-1)\n",
    "        mainft_leaf_cnt[kv_str] += 1\n",
    "\n",
    "ft_class = np.array(ft_class)\n",
    "main_ft_idx = np.where(ft_class == -1)[0]\n",
    "traffic_ft_idx = np.where(ft_class == 1)[0]\n",
    "\n",
    "# area of interest\n",
    "changepoint = tstp[0] + timestamp\n",
    "win_idx = np.where(abs(tstp - changepoint) <= one_sided_window)[0]\n",
    "# in window data preprocessing, take care of the numpy view and copy in slicing the data\n",
    "cp_window = data[win_idx,:]\n",
    "data_slice_shape = minmax(adaptive_diff(cp_window))  # cares only about the shape\n",
    "data_slice_raw = adaptive_diff(cp_window)\n",
    "\n",
    "# sorting metrics on a per counter/feature base\n",
    "main_metric = metric_func(data_slice_shape[:, main_ft_idx])\n",
    "traffic_metric = metric_func(data_slice_raw[:, traffic_ft_idx])\n",
    "\n",
    "sorted_main = np.array(sorted(enumerate(main_metric.tolist()), key=lambda s: s[1], reverse=True))\n",
    "sorted_traffic = np.array(sorted(enumerate(traffic_metric.tolist()), key=lambda s: s[1], reverse=True))\n",
    "\n",
    "if len(sorted_traffic) > 0:\n",
    "    traffic_ft_names = [ft_names_idx[i] for i in traffic_ft_idx]\n",
    "    t_names = [traffic_ft_names[i] for i in sorted_traffic[:,0].astype(int)]\n",
    "    traffic_scores = sorted_traffic[:,1] / max(sorted_traffic[:,1])\n",
    "else:\n",
    "    t_names = []\n",
    "    traffic_scores = np.array([])\n",
    "\n",
    "if len(sorted_main) > 0:\n",
    "    main_ft_names = [ft_names_idx[i] for i in main_ft_idx]\n",
    "    m_names = [main_ft_names[i] for i in sorted_main[:,0].astype(int)]\n",
    "    main_scores = sorted_main[:,1]\n",
    "else:\n",
    "    m_names = []\n",
    "    main_scores = np.array([])\n",
    "\n",
    "full_names = m_names + t_names\n",
    "scores = np.concatenate((main_scores, traffic_scores))\n",
    "\n",
    "selection = run_opti(ft_store, full_names, scores, alpha=regularization_term, \n",
    "                        N_max_epochs=max_number_of_epochs)\n",
    "\n",
    "features = []\n",
    "for x in range(0, min(len(selection), max_selection_size)):\n",
    "    cp_feature = ':'.join(ft_store.get_flat_name(selection[x]).split(':')[1:]) + \" CHANGE: \" \n",
    "    cp_feature +=  str(cp_window[:, selection[x]][-1] - cp_window[:, selection[x]][0])\n",
    "    features.append(cp_feature)\n",
    "\n",
    "utils.displayDictionary({'Features': '\\n'.join(features)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-diagnosis",
   "language": "python",
   "name": "ai-diagnosis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
